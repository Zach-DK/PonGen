{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 2e-4\n",
    "        self.num_epochs = 10\n",
    "        self.save_every = 10\n",
    "        self.frames_dir = \"frames\"\n",
    "        self.white_weight = 4.0  # upweight white pixels\n",
    "        self.num_workers = 0\n",
    "        self.pin_memory = torch.cuda.is_available()\n",
    "\n",
    "config = TrainingConfig()\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d35979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongSequenceDataset(Dataset):\n",
    "    def __init__(self, frames_dir=\"frames\", sequence_length=3):\n",
    "        self.frames_dir = Path(frames_dir)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_files = sorted([f for f in self.frames_dir.glob(\"*.png\")])\n",
    "        self.sequences = self._group_sequences()\n",
    "\n",
    "        print(f\"Found {len(self.frame_files)} frames\")\n",
    "        print(f\"Sequences: {len(self.sequences)}\")\n",
    "\n",
    "    def _group_sequences(self):\n",
    "        sequences, session, prefix = [], [], None\n",
    "        for f in self.frame_files:\n",
    "            parts = f.stem.split('_')\n",
    "            if len(parts) >= 3:\n",
    "                p = '_'.join(parts[:-1])\n",
    "                if prefix is None or p == prefix:\n",
    "                    session.append(f); prefix = p\n",
    "                else:\n",
    "                    if len(session) >= self.sequence_length:\n",
    "                        for i in range(len(session) - self.sequence_length + 1):\n",
    "                            sequences.append(session[i:i+self.sequence_length])\n",
    "                    session, prefix = [f], p\n",
    "        if len(session) >= self.sequence_length:\n",
    "            for i in range(len(session) - self.sequence_length + 1):\n",
    "                sequences.append(session[i:i+self.sequence_length])\n",
    "        return sequences\n",
    "\n",
    "    def __len__(self): return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        frames = []\n",
    "        for f in seq:\n",
    "            img = Image.open(f).convert(\"L\")\n",
    "            arr = np.array(img, dtype=np.float32) / 255.0\n",
    "            frames.append(torch.from_numpy(arr).unsqueeze(0))  # [1, H, W]\n",
    "        prev_, curr_, target = frames[0], frames[1], frames[2]\n",
    "        inp = torch.cat([prev_, curr_], dim=0)  # [2, H, W]\n",
    "        return inp, target\n",
    "\n",
    "# Build loaders\n",
    "train_ds = PongSequenceDataset(config.frames_dir, sequence_length=3)\n",
    "train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True,\n",
    "                          num_workers=config.num_workers, pin_memory=config.pin_memory)\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Batch shapes: input {x.shape} (B,2,200,200), target {y.shape} (B,1,200,200)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876da749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_ch=2, embed_dim=384, patch_size=10, img_size=200):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.grid = img_size // patch_size\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, self.grid*self.grid, embed_dim))  # learned pos\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,2,200,200]\n",
    "        x = self.proj(x)              # [B, C, 20, 20]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, 400, C]\n",
    "        x = x + self.pos\n",
    "        return x  # tokens\n",
    "\n",
    "class ViTNextFrame(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer next-frame predictor\n",
    "    - Input: two frames [t-1, t] in [0,1]\n",
    "    - Output: t+1 via residual added to latest frame\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=200, patch_size=10, in_chans=2,\n",
    "                 embed_dim=384, depth=6, num_heads=8, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.patch = PatchEmbed(in_chans, embed_dim, patch_size, img_size)\n",
    "        self.grid = img_size // patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                                   dim_feedforward=int(embed_dim*mlp_ratio),\n",
    "                                                   batch_first=True, norm_first=True, dropout=0.0)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # control conditioning from bottom row of latest frame (200 dims -> embed_dim)\n",
    "        self.ctrl_mlp = nn.Sequential(\n",
    "            nn.Linear(200, 128), nn.ReLU(),\n",
    "            nn.Linear(128, embed_dim), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # token to patch-pixels\n",
    "        self.head = nn.Linear(embed_dim, patch_size*patch_size)\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def unpatchify(self, tokens):\n",
    "        # tokens: [B, N=400, P*P] -> [B, 1, 200, 200]\n",
    "        B, N, PP = tokens.shape\n",
    "        P = self.patch_size\n",
    "        G = self.grid\n",
    "        x = tokens.view(B, G, G, P, P)      # [B, 20,20,10,10]\n",
    "        x = x.permute(0,1,3,2,4).contiguous().view(B, 1, G*P, G*P)\n",
    "        return x\n",
    "\n",
    "    def forward(self, two_frames):\n",
    "        # two_frames: [B,2,200,200], in [0,1]\n",
    "        B = two_frames.size(0)\n",
    "        latest = two_frames[:, 1:2]  # [B,1,200,200]\n",
    "\n",
    "        # control conditioning from bottom row of latest frame\n",
    "        bottom = latest[:, 0, -1, :]         # [B,200]\n",
    "        ctrl_vec = self.ctrl_mlp(bottom)     # [B,embed_dim]\n",
    "\n",
    "        tokens = self.patch(two_frames)      # [B,400,embed_dim]\n",
    "        # add control bias to every token\n",
    "        tokens = tokens + ctrl_vec.unsqueeze(1)\n",
    "\n",
    "        tokens = self.encoder(tokens)        # [B,400,embed_dim]\n",
    "        pix = self.head(tokens)              # [B,400, P*P]\n",
    "        residual = torch.tanh(pix)           # [-1,1] per pixel\n",
    "        residual_img = self.unpatchify(residual)  # [B,1,200,200]\n",
    "        out = torch.clamp(latest + residual_img, 0.0, 1.0)\n",
    "        return out\n",
    "\n",
    "# Build model\n",
    "model = ViTNextFrame().to(device)\n",
    "print(\"Model params:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_l1_loss(pred: torch.Tensor, tgt: torch.Tensor, white_weight: float = 4.0):\n",
    "    # Upweight bright pixels; includes bottom row\n",
    "    w = 1.0 + white_weight * tgt\n",
    "    return torch.mean(w * torch.abs(pred - tgt))\n",
    "\n",
    "@torch.inference_mode()\n",
    "def visualize_batch(model, loader, max_cols=4):\n",
    "    model.eval()\n",
    "    x, y = next(iter(loader))\n",
    "    x = x.to(device); y = y.to(device)\n",
    "    yhat = model(x)\n",
    "\n",
    "    x = x.cpu(); y = y.cpu(); yhat = yhat.cpu()\n",
    "    cols = min(max_cols, x.shape[0])\n",
    "    fig, axes = plt.subplots(3, cols, figsize=(4*cols, 9))\n",
    "    if cols == 1: axes = axes.reshape(3,1)\n",
    "    for i in range(cols):\n",
    "        axes[0,i].imshow(x[i,1], cmap=\"gray\", vmin=0, vmax=1); axes[0,i].set_title(\"Input t\"); axes[0,i].axis('off')\n",
    "        axes[1,i].imshow(y[i,0], cmap=\"gray\", vmin=0, vmax=1); axes[1,i].set_title(\"Target t+1\"); axes[1,i].axis('off')\n",
    "        axes[2,i].imshow(yhat[i,0], cmap=\"gray\", vmin=0, vmax=1); axes[2,i].set_title(\"Pred\"); axes[2,i].axis('off')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def train(model, loader, cfg: TrainingConfig):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, betas=(0.9, 0.95), weight_decay=0.01)\n",
    "    model.train()\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        running = 0.0\n",
    "        for i, (x,y) in enumerate(loader):\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            opt.zero_grad()\n",
    "            yhat = model(x)\n",
    "            loss = weighted_l1_loss(yhat, y, cfg.white_weight)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += loss.item()\n",
    "            if i % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{cfg.num_epochs} | Batch {i}/{len(loader)} | Loss {loss.item():.4f}\")\n",
    "        print(f\"Epoch {epoch+1} avg loss: {running/len(loader):.4f}\")\n",
    "\n",
    "        if (epoch + 1) % cfg.save_every == 0:\n",
    "            torch.save(model.state_dict(), f\"pong_vit_epoch_{epoch+1}.pth\")\n",
    "            print(f\"Saved: pong_vit_epoch_{epoch+1}.pth\")\n",
    "    torch.save(model.state_dict(), \"pong_vit_final.pth\")\n",
    "    print(\"Saved final: pong_vit_final.pth\")\n",
    "\n",
    "print(\"Ready: call train(model, train_loader, config)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30604f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTInference:\n",
    "    def __init__(self, model: ViTNextFrame, model_path: str|None = None):\n",
    "        self.model = model.to(device).eval()\n",
    "        if model_path and Path(model_path).exists():\n",
    "            try:\n",
    "                state = torch.load(model_path, map_location=device)\n",
    "                # raw state_dict or checkpoint dict support\n",
    "                if isinstance(state, dict) and \"model_state_dict\" in state:\n",
    "                    self.model.load_state_dict(state[\"model_state_dict\"])\n",
    "                else:\n",
    "                    self.model.load_state_dict(state)\n",
    "                print(f\"Loaded model: {model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load model ({model_path}): {e}\")\n",
    "\n",
    "        # warmup\n",
    "        with torch.inference_mode():\n",
    "            for _ in range(20):\n",
    "                _ = self.model(torch.rand(1,2,200,200, device=device))\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict_next(self, prev_np: np.ndarray, curr_np: np.ndarray):\n",
    "        prev_t = torch.from_numpy(prev_np.astype(np.float32)/255.0).unsqueeze(0).unsqueeze(0)\n",
    "        curr_t = torch.from_numpy(curr_np.astype(np.float32)/255.0).unsqueeze(0).unsqueeze(0)\n",
    "        x = torch.cat([prev_t, curr_t], dim=1).to(device)\n",
    "        yhat = self.model(x)[0,0].clamp(0,1).cpu().numpy()\n",
    "        return (yhat * 255.0).astype(np.uint8)\n",
    "\n",
    "def find_model_path(default=\"pong_vit_final.pth\"):\n",
    "    p = Path(default)\n",
    "    if p.exists(): return str(p)\n",
    "    cks = sorted(glob.glob(\"pong_vit_epoch_*.pth\"))\n",
    "    return cks[-1] if cks else None\n",
    "\n",
    "def get_seed(frames_dir=\"frames\"):\n",
    "    if os.path.isdir(frames_dir):\n",
    "        pngs = sorted([p for p in os.listdir(frames_dir) if p.endswith(\".png\")])\n",
    "        if pngs:\n",
    "            img = Image.open(os.path.join(frames_dir, pngs[0])).convert(\"L\")\n",
    "            return np.array(img, dtype=np.uint8)\n",
    "    # fallback synthetic\n",
    "    fr = np.zeros((200,200), dtype=np.uint8)\n",
    "    fr[82:118, 10:16] = 255; fr[82:118, 184:190] = 255; fr[97:103, 97:103] = 255; fr[-1,:] = 0\n",
    "    return fr\n",
    "\n",
    "def generate_gameplay_vit(num_frames=1200, fps=60, output_dir=None, model_path=None, save_video=True):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    outdir = Path(output_dir) if output_dir else Path(f\"generated_vit_{ts}\")\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_path = model_path or find_model_path()\n",
    "    engine = ViTInference(ViTNextFrame(), model_path)\n",
    "\n",
    "    prev = get_seed()\n",
    "    curr = prev.copy()\n",
    "\n",
    "    writer = None\n",
    "    if save_video:\n",
    "        try:\n",
    "            import imageio\n",
    "            writer = imageio.get_writer(outdir / \"gameplay.mp4\", fps=fps)\n",
    "        except Exception as e:\n",
    "            print(f\"Video disabled ({e}). Install with: pip install imageio[ffmpeg]\")\n",
    "\n",
    "    print(f\"Generating {num_frames} frames to {outdir}\")\n",
    "    \n",
    "    # Save and include the seed frame in video\n",
    "    Image.fromarray(curr).save(outdir / f\"frame_{0:05d}.png\")\n",
    "    if writer: writer.append_data(curr)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        nxt = engine.predict_next(prev, curr)\n",
    "        Image.fromarray(nxt).save(outdir / f\"frame_{i+1:05d}.png\")\n",
    "        if writer: writer.append_data(nxt)\n",
    "        prev, curr = curr, nxt\n",
    "        if (i+1) % 100 == 0: print(f\"{i+1}/{num_frames}\")\n",
    "\n",
    "    if writer: writer.close()\n",
    "    print(\"Done.\")\n",
    "    return str(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb52d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train:\n",
    "train(model, train_loader, config)\n",
    "\n",
    "# Visual check:\n",
    "visualize_batch(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = generate_gameplay_vit(num_frames=1800, fps=60, model_path=\"pong_vit_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
